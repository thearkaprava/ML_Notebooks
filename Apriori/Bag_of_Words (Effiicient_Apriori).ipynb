{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset: http://archive.ics.uci.edu/ml/datasets/Bag+of+Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from efficient_apriori import apriori\n",
    "import time\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequent(k,f,name):\n",
    "   \n",
    "    out = open(\"output.txt\", \"a+\")\n",
    "    start = time.time()\n",
    "    process = psutil.Process(os.getpid())\n",
    "    os.chdir('/home/arkaprava/ML/')\n",
    "    \n",
    "    #Reading the files\n",
    "    voc_raw = pd.read_csv('vocab.'+str(name)+'.txt',header= None)\n",
    "    doc_raw = pd.read_csv('docword.'+str(name)+'.txt',header= None)\n",
    "    doc_temp = doc_raw.loc[3:]\n",
    "    \n",
    "    #Docword of the chosen Text Collection\n",
    "    doc = pd.DataFrame(doc_temp[0].str.split(' ',2).tolist(),columns=['Doc_id','Word_id','Count'])  \n",
    "    \n",
    "    #Vocabulary of the chosen Text Collection\n",
    "    voc = voc_raw\n",
    "    voc[\"Word_id\"] = range(1,voc.shape[0]+1)  \n",
    "    voc.columns = ['words','Word_id']\n",
    "      \n",
    "    # Creating dictionary with Word_id as keys and words as values\n",
    "    dic_voc={}\n",
    "    for i in range(0,voc.shape[0]):\n",
    "        dic_voc[i+1] = voc['words'][i]      \n",
    "    \n",
    "    # Creating a list of lists(bow) consisting of Word_ids of Doc_id i  in ith list\n",
    "    dic_doc = doc.groupby('Doc_id')['Word_id'].apply(list).to_dict()\n",
    "    bow=[]\n",
    "    for i in dic_doc.keys():\n",
    "      bow.append(dic_doc[i])\n",
    "    ans=[]\n",
    "    \n",
    "    # Apriori \n",
    "    itemsets, _ = apriori(bow, min_support=f)\n",
    "    try:\n",
    "     for key in itemsets[k].keys():\n",
    "            key = list(key)\n",
    "            for i in range(len(key)):\n",
    "              key[i] = dic_voc[int(key[i])]     \n",
    "            ans.append(tuple(key))\n",
    "    except KeyError:\n",
    "            print(\"[]\", file = out)\n",
    "    end = time.time()    \n",
    "    \n",
    "    # Outputs       \n",
    "      \n",
    "    print(\"\\nName of Text Collection:\",str(name).capitalize(), file = out)\n",
    "    print(\"\\nShape of the Document set of\",str(name).capitalize(),\":\",doc.shape, file = out)\n",
    "    print(\"\\nShape of the Vocabulary set of\",str(name).capitalize(),\":\",voc.shape, file = out)\n",
    "    print(\"\\nHere are the all\",k,\"-itemset of\",str(name).capitalize(),'with minimum support',f,':', file=out)     \n",
    "    print(\"\\n\",ans, file=out)\n",
    "    print('\\nComputation time:',(end-start),'Seconds', file=out)     \n",
    "    print(\"\\nTotal memory used:\",process.memory_info().rss,\"bytes\", file=out)\n",
    "    print(\"\\n\\n ###############################################################\\n\\n\",file = out)\n",
    "    out.close()\n",
    "   \n",
    "frequent(2,0.2,\"kos\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
